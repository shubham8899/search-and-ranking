{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQ2LjE0rTB4i"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "6Z7ScFbfmuLG"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import csv\n",
        "import pickle\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq1VWi0LTJNe"
      },
      "source": [
        "### Defining Index/InvertedIndex Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GLGZUYRgJhh1"
      },
      "outputs": [],
      "source": [
        "def read_documents(path):\n",
        "    f = open(path, 'r')\n",
        "    documents = f.read()\n",
        "    documents = documents.split(\".I \")\n",
        "    for i in range(1,len(documents)):\n",
        "        documents[i] = documents[i].split(\"\\n.S\", maxsplit=1)\n",
        "    return documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "j0huOAvrJjav"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(text,stpwrd):\n",
        "    text =  ' '.join([word for word in text.split() if word not in stpwrd]) # delete stopwords from text\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HZq0Zh2VJkyl"
      },
      "outputs": [],
      "source": [
        "def process_text(text):\n",
        "\tpattern = re.compile('[\\W_]+')\n",
        "\ttext = text.lower()\n",
        "\ttext = pattern.sub(' ', text)\n",
        "\ttext = re.sub(r'[\\W_]+',' ', text)\n",
        "\ttext = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", text)\n",
        "\ttext = text.split()\n",
        "\treturn text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "e9hrmNIWJmso"
      },
      "outputs": [],
      "source": [
        "def clean_documents(documents, stpwrd):\n",
        "    cleaned_documents = {}\n",
        "    for i in range(1,len(documents)-1):\n",
        "        try:\n",
        "            id = documents[i][0][9:]\n",
        "            text = documents[i][1]\n",
        "        except:\n",
        "            continue\n",
        "        text = remove_stopwords(text, stpwrd)\n",
        "        text = process_text(text)\n",
        "        cleaned_documents[int(id)]= text\n",
        "    return cleaned_documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EynwjP1IJokl"
      },
      "outputs": [],
      "source": [
        "def index_one_file(termlist):\n",
        "\tfileIndex = {}\n",
        "\tfor index, word in enumerate(termlist):\n",
        "\t\tif word in fileIndex.keys():\n",
        "\t\t\tfileIndex[word].append(index)\n",
        "\t\telse:\n",
        "\t\t\tfileIndex[word] = [index]\n",
        "\treturn fileIndex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3WqqdB1PJsP_"
      },
      "outputs": [],
      "source": [
        "def make_indices(termlists):\n",
        "\ttotal = {}\n",
        "\tfor filename in termlists.keys():\n",
        "\t\ttotal[filename] = index_one_file(termlists[filename])\n",
        "\treturn total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "piSM9RcqMiea"
      },
      "outputs": [],
      "source": [
        "def fullIndex(regdex):\n",
        "\ttotal_index = {}\n",
        "\tfor filename in regdex.keys():\n",
        "\t\tfor word in regdex[filename].keys():\n",
        "\t\t\tif word in total_index.keys():\n",
        "\t\t\t\tif filename in total_index[word].keys():\n",
        "\t\t\t\t\ttotal_index[word][filename].extend(regdex[filename][word][:])\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\ttotal_index[word][filename] = regdex[filename][word]\n",
        "\t\t\telse:\n",
        "\t\t\t\ttotal_index[word] = {filename: regdex[filename][word]}\n",
        "\treturn total_index"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Index/InvertedIndex, Saving Pickles (Do not run, load from pickles instead)"
      ],
      "metadata": {
        "id": "l9XJZmQUgZ5S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MpuQoe0Jufy",
        "outputId": "01978040-7424-4d3b-ec57-1b86503c8ccb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "### MAIN\n",
        "nltk.download('stopwords')\n",
        "stpwrd = nltk.corpus.stopwords.words('english')\n",
        "stpwrd.extend(['.U', '.S','.M','.T','.P','.W','.M','.I'])\n",
        "\n",
        "documents = read_documents('/content/drive/MyDrive/INFORMATION RETRIEVAL/HW1 SEARCH ENGINE/ohsumed.88-91')\n",
        "cleaned_documents = clean_documents(documents, stpwrd)\n",
        "index = make_indices(cleaned_documents)\n",
        "inverted_index = fullIndex(index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNsUtqBgNGVJ"
      },
      "outputs": [],
      "source": [
        "inverted_index_file = open(\"/content/drive/MyDrive/INFORMATION RETRIEVAL/HW1 SEARCH ENGINE/inverted_index.pkl\", \"wb\")\n",
        "pickle.dump(inverted_index, inverted_index_file)\n",
        "inverted_index_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_file = open(\"/content/drive/MyDrive/INFORMATION RETRIEVAL/HW1 SEARCH ENGINE/index.pkl\", \"wb\")\n",
        "pickle.dump(index, index_file)\n",
        "index_file.close()"
      ],
      "metadata": {
        "id": "qjwrvPv5e6K2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZfNoOmSTPks"
      },
      "source": [
        "### Loading Index/InvertedIndex from Pickles"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b_file = open(\"/content/drive/MyDrive/INFORMATION RETRIEVAL/HW1 SEARCH ENGINE/index.pkl\", \"rb\")\n",
        "index = pickle.load(b_file)"
      ],
      "metadata": {
        "id": "UnJhrJo-gyTR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "btUuzea8Nk3a"
      },
      "outputs": [],
      "source": [
        "a_file = open(\"/content/drive/MyDrive/INFORMATION RETRIEVAL/HW1 SEARCH ENGINE/inverted_index.pkl\", \"rb\")\n",
        "inverted_index = pickle.load(a_file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# index[88143308]"
      ],
      "metadata": {
        "id": "g0rHLOvQhcgQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inverted_index['neodymium']"
      ],
      "metadata": {
        "id": "XtWSryc0hksM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFCIz20JWRmq"
      },
      "source": [
        "### Query Parsing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "VQHsAAegkke1"
      },
      "outputs": [],
      "source": [
        "def read_queries(filename):\n",
        "    queries = []\n",
        "    with open(filename, 'r+') as f:\n",
        "        current_query = None\n",
        "        for line in f:\n",
        "            line = line[:-1]\n",
        "            if '<top>' in line:\n",
        "                current_query = {}\n",
        "            elif '</top>' in line:\n",
        "                queries.append(current_query)\n",
        "                current_query = {}\n",
        "            elif '<num>' in line:\n",
        "                current_query['num'] = line.split(':')[1].strip()\n",
        "            elif '<title>' in line:\n",
        "                current_query['title'] = line.split('>')[1].strip()\n",
        "            elif (not '<desc>' in line and len(line) > 2):\n",
        "                current_query['description'] = line\n",
        "\n",
        "            queries_2 = {}\n",
        "\n",
        "            for i in range(len(queries)):\n",
        "                queries_2[queries[i]['num']] = queries[i]['description']\n",
        "\n",
        "    return queries_2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_queries(queries, stpwrd):\n",
        "    for i, j in queries.items():\n",
        "        text = queries[i]\n",
        "        text = remove_stopwords(text, stpwrd)\n",
        "        text = process_text(text)\n",
        "        queries[i] = text\n",
        "    return queries"
      ],
      "metadata": {
        "id": "NVVfa-cebP6d"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def free_text_query(query):\n",
        "    terms = query\n",
        "    docs=set()\n",
        "    for term in terms:\n",
        "        try:\n",
        "            termDocs = [i for i in inverted_index[term]]\n",
        "            # print(termDocs)\n",
        "            docs |= set(termDocs)\n",
        "        except:\n",
        "            #term is not in inverted index\n",
        "            pass\n",
        "    docs=list(docs)\n",
        "    return docs"
      ],
      "metadata": {
        "id": "Vf5Tzm_6XaG9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def intersection(lst1, lst2):\n",
        "    return list(set(lst1) & set(lst2))"
      ],
      "metadata": {
        "id": "8PbtnQL1wdoP"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Queries"
      ],
      "metadata": {
        "id": "dv5kzzcKQZp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stpwrd = nltk.corpus.stopwords.words('english')\n",
        "stpwrd.extend(['.U', '.S','.M','.T','.P','.W','.M','.I'])\n",
        "queries = read_queries('/content/drive/MyDrive/INFORMATION RETRIEVAL/HW1 SEARCH ENGINE/query.ohsu.1-63')\n",
        "queries = clean_queries(queries, stpwrd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltMlF7-m5qSA",
        "outputId": "6be3fb67-9883-4f43-dfbb-7f91697c238a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ranking Functions"
      ],
      "metadata": {
        "id": "t2jChWonP7Op"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def boolean_ranking(docs, query):\n",
        "    docs_score = {}\n",
        "    for doc in docs:\n",
        "        score = 0\n",
        "        score = len ( intersection(list(index[doc].keys()), query) )\n",
        "        docs_score[doc] = score\n",
        "    docs_score = list( sorted(docs_score.items(),\n",
        "                           key=lambda item: item[1],\n",
        "                           reverse=True))\n",
        "    return docs_score[:50]"
      ],
      "metadata": {
        "id": "d3M0pkd-peDr"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_ranking(docs, query):\n",
        "    docs_score = {}\n",
        "    for doc in docs:\n",
        "        doc_length = len(index[doc])\n",
        "        absolute_freq = 0\n",
        "        normalized_freq = 0\n",
        "        for term in query:\n",
        "            try:\n",
        "                term_freq = len(index[doc][term])\n",
        "            except:\n",
        "                term_freq = 0\n",
        "            absolute_freq += term_freq\n",
        "        normalized_freq = absolute_freq / doc_length\n",
        "        docs_score[doc] = normalized_freq\n",
        "    docs_score = list( sorted(docs_score.items(),\n",
        "                           key=lambda item: item[1],\n",
        "                           reverse=True))\n",
        "    return docs_score[:50]\n"
      ],
      "metadata": {
        "id": "yv1pLXQvByxU"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_idf_ranking(docs, query):\n",
        "    docs_score = {}\n",
        "    for doc in docs:\n",
        "        doc_length = len(index[doc])\n",
        "        absolute_freq = 0\n",
        "        normalized_freq = 0\n",
        "        idf_total = 0\n",
        "        for term in query:\n",
        "            try:\n",
        "                term_freq = len(index[doc][term])\n",
        "            except:\n",
        "                term_freq = 0\n",
        "            absolute_freq += term_freq\n",
        "        normalized_freq = absolute_freq / doc_length\n",
        "\n",
        "        for term in query:\n",
        "            try:\n",
        "                document_freq = len(inverted_index[term])\n",
        "            except:\n",
        "                continue\n",
        "            idf = len(inverted_index) / document_freq\n",
        "            idf = math.log(idf)\n",
        "            idf_total += idf\n",
        "\n",
        "        tf_idf = normalized_freq * idf_total\n",
        "\n",
        "        docs_score[doc] = tf_idf\n",
        "    docs_score = list( sorted(docs_score.items(),\n",
        "                           key=lambda item: item[1],\n",
        "                           reverse=True))\n",
        "    return docs_score[:50]"
      ],
      "metadata": {
        "id": "rS3gJ6ZpRCUz"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_ranking(docs, query):\n",
        "    docs_score = tf_idf_ranking(docs, query)\n",
        "    new_docs = []\n",
        "    for i, j in docs_score:\n",
        "        new_docs.append(i)\n",
        "    new_docs_score = boolean_ranking(new_docs, query)\n",
        "    return new_docs_score"
      ],
      "metadata": {
        "id": "RyYiFgULxRIS"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Log Files"
      ],
      "metadata": {
        "id": "jqhHrx6ZQPYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_log_file(algorithm_name):\n",
        "    f = open(algorithm_name, 'w')\n",
        "    for query_code, query in queries.items():\n",
        "        docs = free_text_query(query)\n",
        "        if algorithm_name == 'BOOL':\n",
        "            docs_score = boolean_ranking(docs, query)\n",
        "        elif algorithm_name == 'TF':\n",
        "            docs_score = tf_ranking(docs, query)\n",
        "        elif algorithm_name == 'TF-IDF':\n",
        "            docs_score = tf_idf_ranking(docs, query)\n",
        "        elif algorithm_name == 'CUSTOM':\n",
        "            docs_score = custom_ranking(docs, query)\n",
        "        for i in range(len(docs_score)):\n",
        "            f.write(query_code + \"\\tQ0\\t\" + str(docs_score[i][0]) + \"\\t\" \n",
        "                    + str(i+1) + \"\\t\" + str(docs_score[i][1]) + \"\\t\" + algorithm_name +\"\\n\")\n",
        "    f.close()"
      ],
      "metadata": {
        "id": "0LyDX9vrIxXP"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_log_file('BOOL')"
      ],
      "metadata": {
        "id": "wAXb2Rae7ogY"
      },
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_log_file('TF')"
      ],
      "metadata": {
        "id": "TU_867L4PtsQ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_log_file('TF-IDF')"
      ],
      "metadata": {
        "id": "E3mQ3RC4PvIA"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_log_file('CUSTOM')"
      ],
      "metadata": {
        "id": "EfuGzmFF5N0i"
      },
      "execution_count": 57,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "IR HW1 Search Engine",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}